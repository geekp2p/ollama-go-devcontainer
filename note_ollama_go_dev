curl -X POST http://localhost:8082/chat \
-H "Content-Type: application/json" \
-d '{"prompt":"สรุปแนวคิดของ Expected Value ในการลงทุนหน่อย"}'



---


## How to use with your own repo name
- Search/replace `github.com/yourname/ollama-go-devcontainer` in `go.mod` and imports.


---


## Bonus: simple streaming (optional)
If you want server‑sent tokens, switch to `/api/generate` with `stream:true` and flush chunks; or upgrade to websockets. This template keeps it simple (single non‑streaming reply) so you can extend as needed.


docker exec -it ollama ollama pull gpt-oss:20b

make run
# หรือ
go run ./cmd/server


curl -X POST http://localhost:8082/chat \
  -H "Content-Type: application/json" \
  -d '{"prompt":"สรุป Expected Value ในการลงทุนหน่อย"}'





# นอกคอนเทนเนอร์ (Windows/PowerShell)
docker compose down -v

# VS Code
F1 → Dev Containers: Rebuild and Reopen in Container (เลือก no cache)

# ในคอนเทนเนอร์ app
go mod tidy
make run      # หรือ go run ./cmd/server

# ทดสอบจาก host
curl -X POST http://localhost:8082/chat ^
  -H "Content-Type: application/json" ^
  -d "{\"prompt\":\"สรุป Expected Value แบบสั้นๆ\"}"



# 0) สร้างโฟลเดอร์โมเดลในโปรเจกต์
mkdir models

# 1) ปิดของเดิมถ้ามี
docker compose down -v

# 2) เปิดใหม่
docker compose up -d

# 3) ดึงโมเดล (เก็บลง ./models อัตโนมัติ)
docker exec -it ollama ollama pull gpt-oss:20b

# 4) เข้า devcontainer แล้วรันแอป
# VS Code: Dev Containers: Reopen in Container
go mod tidy
make run   # หรือ go run ./cmd/server



docker compose up -d ollama
docker exec -it ollama ollama pull gpt-oss:20b




docker exec -it ollama ollama pull gpt-oss:20b

docker exec -it ollama ollama run gpt-oss:20b "hello"

docker exec -it ollama ollama list

docker compose up -d app
curl -X POST http://localhost:8082/chat -H "Content-Type: application/json" -d "{\"prompt\":\"สวัสดี\"}"


docker compose up -d ollama

docker exec -it ollama ollama pull gpt-oss:20b

wsl --shutdown

Get-Process "Docker Desktop" -ErrorAction SilentlyContinue | Stop-Process -Force

docker context ls
docker context use default   # หรือ desktop-linux ถ้ามี
docker version               # ต้องเห็นทั้ง Client/Server




cd C:\ollama-go-devcontainer
docker compose build --no-cache app
docker compose up -d


docker exec -it ollama ollama list

curl.exe -X POST http://localhost:8082/chat -H "Content-Type: application/json" -d "{\"prompt\":\"สวัสดี\"}"


docker compose exec go-app sh

docker compose up -d

docker compose exec app sh

docker exec -it go-app sh

go run ./cmd/server

docker compose up -d ollama
docker exec -it ollama ollama pull gpt-oss:20b

; docker exec -it ollama ollama pull openthaigpt1.5-7b-instruct
; docker exec -it ollama ollama pull openthaigpt1.5-14b-instruct
docker exec -it ollama ollama pull promptnow/openthaigpt1.5-14b-instruct-q4_k_m

docker exec -it ollama ollama pull llama3.2-vision:11b
docker exec -it ollama ollama pull internvl2
docker exec -it ollama ollama pull internvl2.5


C:\ollama-go-devcontainer>docker exec -it ollama ollama list
NAME                                                   ID              SIZE      MODIFIED
llama3.2-vision:11b                                    6f2f9757ae97    7.8 GB    3 hours ago
promptnow/openthaigpt1.5-14b-instruct-q4_k_m:latest    e970d4406978    9.0 GB    4 hours ago
gpt-oss:20b                                            aa4295ac10c3    13 GB     6 hours ago

C:\ollama-go-devcontainer>

docker exec -it ollama ollama run promptnow/openthaigpt1.5-14b-instruct-q4_k_m "สวัสดี"

docker exec -it ollama ollama run llama3.2-vision:11b "แนะนำตัวหน่อย"
docker exec -it ollama ollama run gpt-oss:20b "อธิบายเรื่องดอกกุหลาบ"

docker exec -it ollama ollama run llama3.2-vision:11b "อธิบายภาพนี้ให้หน่อย ./img/vision_test.jpg"

